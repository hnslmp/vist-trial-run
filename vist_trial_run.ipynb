{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Fourth of July prerequisite, ID: 694227468, URL: https://farm2.staticflickr.com/1125/694227468_f6c433d7d8_o.jpg\n",
      "Title: Spectacular fireworks, ID: 694227344, URL: https://farm2.staticflickr.com/1330/694227344_58d54d3732_o.jpg\n",
      "Title: Bubba Burgers, ID: 694227412, URL: https://farm2.staticflickr.com/1008/694227412_001b568f92_o.jpg\n",
      "Title: \"On guard!\", ID: 694227488, URL: https://farm2.staticflickr.com/1302/694227488_bb07200c72_o.jpg\n",
      "Title: BBQin', ID: 694227508, URL: https://farm2.staticflickr.com/1307/694227508_df12d3b4fb_o.jpg\n",
      "Description: Bubba burgers, beer and BBQ make for a great Fourth of July, ID: 72157600601428727, Title: Fourth of July 2007\n",
      "Description: , ID: 72157601202851033, Title: WMATA - WDC - 04 July 2007\n",
      "Description: July 4th images from Brandon parade and fireworks., ID: 72157594187408667, Title: Brandon FL 7/4/2006\n",
      "Description: Fireworks and friends!, ID: 72157594234060064, Title: 2006 July 4th\n",
      "Description: just a glimpse into the happenings on and around the day to celebrate our National Independence!, ID: 72157600650275265, Title: Hometown 4th\n",
      "Original Text: My sister arrived early to help me with the family Bar BQ., Story ID: 40470, Photo ID: 693397887\n",
      "Original Text: Every one else arrived soon after., Story ID: 40470, Photo ID: 695160730\n",
      "Original Text: Dad manned the grill., Story ID: 40470, Photo ID: 694227508\n",
      "Original Text: There was so much food and it was all delicious., Story ID: 40470, Photo ID: 693397865\n",
      "Original Text: We ended the day shooting off some fireworks., Story ID: 40470, Photo ID: 694227468\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from types import SimpleNamespace\n",
    "import random\n",
    "import textwrap\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "filename = \"sis/val.story-in-sequence.json\"\n",
    "image_folder = \"images/val\"\n",
    "output_folder = \"story_plots_val_horizontal/\"\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "images_data = data['images']\n",
    "albums_data = data.get('albums', [])\n",
    "annotations_data = data['annotations']\n",
    "\n",
    "images = [json.loads(json.dumps(image), object_hook=lambda d: SimpleNamespace(**d)) for image in images_data]\n",
    "albums = [json.loads(json.dumps(album), object_hook=lambda d: SimpleNamespace(**d)) for album in albums_data]\n",
    "\n",
    "annotations = [[json.loads(json.dumps(item), object_hook=lambda d: SimpleNamespace(**d)) for item in annotation_list] for annotation_list in annotations_data]\n",
    "\n",
    "for img in images[:5]:\n",
    "    print(f\"Title: {img.title}, ID: {img.id}, URL: {img.url_o}\")\n",
    "\n",
    "for album in albums[:5]:\n",
    "    print(f\"Description: {album.description}, ID: {album.id}, Title: {album.title}\")\n",
    "\n",
    "if annotations:\n",
    "    for annotation in annotations[:5]:\n",
    "        annotation = annotation[0]\n",
    "        print(f\"Original Text: {annotation.original_text}, Story ID: {annotation.story_id}, Photo ID: {annotation.photo_flickr_id}\")\n",
    "\n",
    "annotations_dict = {}\n",
    "for annotationArray in annotations:\n",
    "    for annotation in annotationArray:\n",
    "        story_id = annotation.story_id\n",
    "        if story_id not in annotations_dict:\n",
    "            annotations_dict[story_id] = []\n",
    "        annotations_dict[story_id].append(annotation)\n",
    "\n",
    "story_ids = list(annotations_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from PIL import Image\n",
    "\n",
    "def get_story_images(story_id, annotations_dict, images, image_folder):\n",
    "    story_annotations = sorted(\n",
    "        annotations_dict.get(story_id, []),\n",
    "        key=lambda ann: ann.worker_arranged_photo_order\n",
    "    )\n",
    "    story_images = [\n",
    "        img for img in images if img.id in [ann.photo_flickr_id for ann in story_annotations]\n",
    "    ]\n",
    "    image_filenames = []\n",
    "    for image in story_images:\n",
    "        image_path = os.path.join(image_folder, f\"{image.id}.jpg\")\n",
    "        if os.path.exists(image_path):\n",
    "            image_filenames.append(f\"{image.id}.jpg\")\n",
    "        else:\n",
    "            print(\"Files not exist\", image_path)\n",
    "    return image_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "\n",
    "def qwen_test_few_shots(\n",
    "    images_list_of_lists,\n",
    "    prompts,\n",
    "    model_name=\"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    image_dir=\"images/val/\",\n",
    "    resize_to=(224, 224),\n",
    "    device_index=0\n",
    "):\n",
    "    \"\"\"\n",
    "    images_list_of_lists: list of lists of image filenames\n",
    "        E.g. [\n",
    "          [\"img1.jpg\",\"img2.jpg\"],  # Turn 1 images\n",
    "          [\"img3.jpg\"],            # Turn 2 images\n",
    "          ...\n",
    "        ]\n",
    "    \n",
    "    prompts: list of dictionaries with \"role\" and \"text\"\n",
    "        E.g. [\n",
    "          {\"role\": \"system\", \"text\": \"...\"},\n",
    "          {\"role\": \"user\", \"text\": \"...\"},\n",
    "        ]\n",
    "    \"\"\"\n",
    "    device = torch.device(f\"cuda:{device_index}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\",  # or \"sequential\" if you have specific needs\n",
    "        use_cache=False\n",
    "    ).eval()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "    # Build the multi-turn conversation structure\n",
    "    conversation = []\n",
    "    all_processed_images = []\n",
    "\n",
    "    # Make sure images_list_of_lists and prompts have the same length\n",
    "    if len(images_list_of_lists) != len(prompts):\n",
    "        raise ValueError(\n",
    "            f\"Mismatched lengths: got {len(images_list_of_lists)} image-turns \"\n",
    "            f\"and {len(prompts)} prompt-turns.\"\n",
    "        )\n",
    "\n",
    "    for img_filenames, prompt_dict in zip(images_list_of_lists, prompts):\n",
    "        role = prompt_dict.get(\"role\", \"user\")  # default to \"user\" if not provided\n",
    "        text = prompt_dict.get(\"text\", \"\")\n",
    "\n",
    "        # Load images for this turn\n",
    "        turn_images = []\n",
    "        for fn in img_filenames:\n",
    "            path = os.path.join(image_dir, fn)\n",
    "            try:\n",
    "                raw_image = Image.open(path).convert(\"RGB\")\n",
    "                # Use Image.Resampling.LANCZOS if you see a PIL deprecation warning\n",
    "                raw_image = raw_image.resize(resize_to, Image.LANCZOS)\n",
    "                turn_images.append(raw_image)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load image {path} due to error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Add them to the global list of images\n",
    "        all_processed_images.extend(turn_images)\n",
    "\n",
    "        # Create the content block for this turn: \"image\" placeholders + text prompt\n",
    "        turn_content = [{\"type\": \"image\"} for _ in turn_images]\n",
    "        if text:  # If there's text in this turn\n",
    "            turn_content.append({\"type\": \"text\", \"text\": text})\n",
    "\n",
    "        # Append to conversation\n",
    "        conversation.append({\n",
    "            \"role\": role,\n",
    "            \"content\": turn_content\n",
    "        })\n",
    "\n",
    "    if not all_processed_images:\n",
    "        return [], \"No valid images were processed.\"\n",
    "\n",
    "    # Apply Qwen's chat template to the entire multi-turn conversation\n",
    "    text_prompt = processor.apply_chat_template(\n",
    "        conversation,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Prepare model inputs\n",
    "    inputs = processor(\n",
    "        text=[text_prompt],\n",
    "        images=all_processed_images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    inputs = {k: v.to(device, non_blocking=True) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    generated_ids = [\n",
    "        output_id[len(input_id):]\n",
    "        for input_id, output_id in zip(inputs['input_ids'], output_ids)\n",
    "    ]\n",
    "    story_description = processor.batch_decode(\n",
    "        generated_ids,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )[0]\n",
    "    return all_processed_images, story_description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_story_ids = story_ids[:60]\n",
    "\n",
    "unique_ids = []\n",
    "seen_sequences = set()\n",
    "\n",
    "for story_id in temp_story_ids:\n",
    "    story_images = get_story_images(story_id, annotations_dict, images, image_folder)\n",
    "\n",
    "    images_key = tuple(story_images)\n",
    "\n",
    "    if images_key not in seen_sequences:\n",
    "        seen_sequences.add(images_key)\n",
    "        unique_ids.append(story_id)\n",
    "\n",
    "temp_story_ids = unique_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035ea74b566f453183a815f8e9f21d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "/home/adnlp-server/anaconda3/envs/hansel_vist_3_10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/adnlp-server/anaconda3/envs/hansel_vist_3_10/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/adnlp-server/anaconda3/envs/hansel_vist_3_10/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.001` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/adnlp-server/anaconda3/envs/hansel_vist_3_10/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import uuid\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "general_instruction = \"\"\"You are an advanced assistant designed to create a 5-sentence story based on a 5-image sequence input. \n",
    "Your task is to generate the appropriate text for each image input. \n",
    "You are given a 5-sequence image and also an aspect to enhance. \n",
    "This aspect will consist of the aspect name and definition, explaining how to express that particular aspect. \n",
    "Your objective is to generate a 5-sentence story that expresses this aspect while still accurately visualizing the related image. \n",
    "Vary between first-person and third-person viewpoints. \n",
    "You can generate a named entity for the entities detected in the image.\n",
    "\n",
    "Aspect list:\n",
    "1) Immersion: A proper immersion is a story that has a consistent World Building. The world must have its own rules and logic. Ensure the world feels real.\n",
    "2) Structure: A proper structure uses a clear beginning, middle, and end.\n",
    "Generate a story based on the input image\n",
    "\"\"\"\n",
    "#1) Immersion: A proper immersion is a story that has a consistent World Building. The world must have its own rules and logic. Ensure the world feels real.\n",
    "#1) Structure: A proper structure uses a clear beginning, middle, and end.\n",
    "# generation_prompt = (\n",
    "#     \"Generate a story based on the input image\"\n",
    "# )\n",
    "\n",
    "temp_story_ids = story_ids[:60]\n",
    "unique_ids = []\n",
    "seen_sequences = set()\n",
    "\n",
    "for story_id in temp_story_ids:\n",
    "    story_images = get_story_images(story_id, annotations_dict, images, image_folder)\n",
    "    images_key = tuple(story_images)\n",
    "    if images_key not in seen_sequences:\n",
    "        seen_sequences.add(images_key)\n",
    "        unique_ids.append(story_id)\n",
    "\n",
    "temp_story_ids = unique_ids\n",
    "\n",
    "folder_name = \"1_feb_generate_immersion_structure_one_shot\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# Prepare CSV file to export data (3 columns: story_id, prompts, generated_story)\n",
    "csv_filename = os.path.join(folder_name, f\"{folder_name}.csv\")\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"story_id\", \"prompts\", \"generated_story\"])  # CSV header\n",
    "\n",
    "    for final_story_id in temp_story_ids:\n",
    "        sample_pairs = [\n",
    "            # Add or remove any few-shot examples here if needed.\n",
    "        ]\n",
    "\n",
    "        images_list_of_lists = []\n",
    "        prompts = []\n",
    "        prompts.append({\"role\": \"system\", \"text\": general_instruction})\n",
    "        # images_list_of_lists.append([])\n",
    "\n",
    "        for sample in sample_pairs:\n",
    "            sample_images = get_story_images(sample[\"story_id\"], annotations_dict, images, image_folder)\n",
    "            prompts.append({\"role\": \"user\", \"text\": sample[\"question\"]})\n",
    "            images_list_of_lists.append(sample_images)\n",
    "            prompts.append({\"role\": \"assistant\", \"text\": sample[\"answer\"]})\n",
    "            images_list_of_lists.append([])\n",
    "\n",
    "        final_story_images = get_story_images(final_story_id, annotations_dict, images, image_folder)\n",
    "        images_list_of_lists.append(final_story_images)\n",
    "        # prompts.append({\"role\": \"system\", \"text\": generation_prompt})\n",
    "\n",
    "        # Call your function that returns processed_images and the response\n",
    "        processed_images, response = qwen_test_few_shots(\n",
    "            images_list_of_lists=images_list_of_lists,\n",
    "            prompts=prompts,\n",
    "            image_dir=\"images/val\"\n",
    "        )\n",
    "\n",
    "        # Generate figure\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        num_images = len(processed_images)\n",
    "        for i, img in enumerate(processed_images, 1):\n",
    "            plt.subplot(2, num_images, i)\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Image {i}')\n",
    "\n",
    "        # Display the response\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.text(\n",
    "            0.5, 0.7, \"Response: \" + response,\n",
    "            horizontalalignment='center', verticalalignment='center',\n",
    "            wrap=True, fontsize=10, bbox=dict(facecolor='white', alpha=0.5)\n",
    "        )\n",
    "\n",
    "        # Format all prompts the same way you do in the figure\n",
    "        formatted_prompts = \"\\n\".join(\n",
    "            f\"{idx+1}) Role: {entry['role'].capitalize()}, Text: {entry['text']}\"\n",
    "            for idx, entry in enumerate(prompts)\n",
    "        )\n",
    "\n",
    "        plt.text(\n",
    "            0.5, 0.3,\n",
    "            \"Prompt: \" + formatted_prompts,\n",
    "            horizontalalignment='center', verticalalignment='center',\n",
    "            wrap=True, fontsize=10, bbox=dict(facecolor='white', alpha=0.5)\n",
    "        )\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Final Story for Story ID: {final_story_id}')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        combined_filename =  f'story_combined_{timestamp}_{uuid.uuid4().hex}.png'\n",
    "        plt.savefig(os.path.join(folder_name, combined_filename))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # Write row to CSV: Story ID, Formatted Prompts, Generated Story\n",
    "        writer.writerow([final_story_id, formatted_prompts, response])\n",
    "\n",
    "print(f\"Exported data to CSV: {csv_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hansel_vist_3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
