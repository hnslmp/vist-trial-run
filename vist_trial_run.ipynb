{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from types import SimpleNamespace\n",
    "import random\n",
    "import textwrap\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "filename = \"sis/val.story-in-sequence.json\"\n",
    "image_folder = \"images/val\"\n",
    "output_folder = \"story_plots_val_horizontal/\"\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "images_data = data['images']\n",
    "albums_data = data.get('albums', [])\n",
    "annotations_data = data['annotations']\n",
    "\n",
    "images = [json.loads(json.dumps(image), object_hook=lambda d: SimpleNamespace(**d)) for image in images_data]\n",
    "albums = [json.loads(json.dumps(album), object_hook=lambda d: SimpleNamespace(**d)) for album in albums_data]\n",
    "\n",
    "annotations = [[json.loads(json.dumps(item), object_hook=lambda d: SimpleNamespace(**d)) for item in annotation_list] for annotation_list in annotations_data]\n",
    "\n",
    "for img in images[:5]:\n",
    "    print(f\"Title: {img.title}, ID: {img.id}, URL: {img.url_o}\")\n",
    "\n",
    "for album in albums[:5]:\n",
    "    print(f\"Description: {album.description}, ID: {album.id}, Title: {album.title}\")\n",
    "\n",
    "if annotations:\n",
    "    for annotation in annotations[:5]:\n",
    "        annotation = annotation[0]\n",
    "        print(f\"Original Text: {annotation.original_text}, Story ID: {annotation.story_id}, Photo ID: {annotation.photo_flickr_id}\")\n",
    "\n",
    "annotations_dict = {}\n",
    "for annotationArray in annotations:\n",
    "    for annotation in annotationArray:\n",
    "        story_id = annotation.story_id\n",
    "        if story_id not in annotations_dict:\n",
    "            annotations_dict[story_id] = []\n",
    "        annotations_dict[story_id].append(annotation)\n",
    "\n",
    "story_ids = list(annotations_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import textwrap\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image\n",
    "\n",
    "# def plot_story_images_and_annotations(story_id, save_plot=True):\n",
    "#     story_annotations = sorted(\n",
    "#         annotations_dict.get(story_id, []),\n",
    "#         key=lambda ann: ann.worker_arranged_photo_order\n",
    "#     )\n",
    "\n",
    "#     story_images = [\n",
    "#         img for img in images if img.id in [ann.photo_flickr_id for ann in story_annotations]\n",
    "#     ]\n",
    "\n",
    "#     print\n",
    "\n",
    "#     if story_annotations:\n",
    "#         story_tier = story_annotations[0].tier\n",
    "#         story_setting = story_annotations[0].setting\n",
    "#         story_id = story_annotations[0].story_id\n",
    "#         plot_title = f\"{story_id}\\nTier: {story_tier} | Setting: {story_setting}\\n\\n\"\n",
    "#     else:\n",
    "#         plot_title = f\"{story_id}\\nTier: N/A | Setting: N/A\"\n",
    "\n",
    "#     num_images = len(story_images)\n",
    "#     # Set up the figure with 1 row and num_images columns\n",
    "#     fig, axes = plt.subplots(nrows=1, ncols=num_images, figsize=(num_images * 4, 6))\n",
    "\n",
    "#     # Ensure axes is always iterable\n",
    "#     if num_images == 1:\n",
    "#         axes = [axes]\n",
    "\n",
    "#     for ax, annotation in zip(axes, story_annotations):\n",
    "#         image = next(\n",
    "#             (img for img in story_images if img.id == annotation.photo_flickr_id),\n",
    "#             None\n",
    "#         )\n",
    "\n",
    "#         if image:\n",
    "#             image_path = os.path.join(image_folder, f\"{image.id}.jpg\")\n",
    "#             if os.path.exists(image_path):\n",
    "#                 img_data = Image.open(image_path)\n",
    "#                 ax.imshow(img_data)\n",
    "#             else:\n",
    "#                 print(f\"Image file {image_path} not found.\")\n",
    "#                 continue\n",
    "#         else:\n",
    "#             ax.text(\n",
    "#                 0.5, 0.5, \"Image not found\",\n",
    "#                 horizontalalignment='center', verticalalignment='center',\n",
    "#                 transform=ax.transAxes\n",
    "#             )\n",
    "\n",
    "#         ax.axis('off')\n",
    "#         # Removed individual captions to avoid overlap\n",
    "#         # wrapped_text = textwrap.fill(annotation.original_text, width=30)\n",
    "#         # ax.set_xlabel(wrapped_text, fontsize=10)\n",
    "    \n",
    "#     # Collect all captions into one paragraph\n",
    "#     captions = \" \".join([annotation.original_text for annotation in story_annotations])\n",
    "#     # Wrap the text to fit within the figure width\n",
    "#     wrapped_captions = textwrap.fill(captions, width=100)\n",
    "\n",
    "#     fig.suptitle(plot_title, fontsize=14)\n",
    "#     plt.tight_layout()\n",
    "#     # Adjust the spacing to make room for the paragraph at the bottom\n",
    "#     plt.subplots_adjust(bottom=0.2)\n",
    "\n",
    "#     # Add the paragraph below the images\n",
    "#     fig.text(\n",
    "#         0.5,   # X location (centered)\n",
    "#         0.02,  # Y location (2% from the bottom)\n",
    "#         wrapped_captions,\n",
    "#         ha='center',\n",
    "#         va='bottom',\n",
    "#         fontsize=16\n",
    "#     )\n",
    "\n",
    "#     if save_plot:\n",
    "#         output_path = os.path.join(output_folder, f\"{story_id}.png\")\n",
    "#         plt.savefig(output_path, bbox_inches='tight')\n",
    "#         print(f\"Plot saved for Story ID {story_id} at {output_path}\")\n",
    "\n",
    "#     plt.close(fig)  # Close the figure to avoid display in the notebook\n",
    "\n",
    "# plot_story_images_and_annotations(\"40470\")\n",
    "# # Loop through each story_id and save the plot\n",
    "# # for story_id in story_ids:\n",
    "# #     print(story_ids)\n",
    "# #     plot_story_images_and_annotations(story_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from PIL import Image\n",
    "\n",
    "def get_story_images(story_id, annotations_dict, images, image_folder):\n",
    "    story_annotations = sorted(\n",
    "        annotations_dict.get(story_id, []),\n",
    "        key=lambda ann: ann.worker_arranged_photo_order\n",
    "    )\n",
    "    story_images = [\n",
    "        img for img in images if img.id in [ann.photo_flickr_id for ann in story_annotations]\n",
    "    ]\n",
    "    image_filenames = []\n",
    "    for image in story_images:\n",
    "        image_path = os.path.join(image_folder, f\"{image.id}.jpg\")\n",
    "        if os.path.exists(image_path):\n",
    "            image_filenames.append(f\"{image.id}.jpg\")\n",
    "        else:\n",
    "            print(\"Files not exist\", image_path)\n",
    "    return image_filenames\n",
    "\n",
    "def qwen_test_all_in_one(story_images, model_name=\"Qwen/Qwen2-VL-7B-Instruct\", image_dir=\"images/val/\", resize_to=(224, 224), device_index=0, query_tmp=\"print error message\"):\n",
    "    device = torch.device(f\"cuda:{device_index}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"sequential\",\n",
    "        use_cache=False\n",
    "    ).eval()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    processor = AutoProcessor.from_pretrained(model_name)\n",
    "    # query_tmp = \"You will be given a sequence of images. For each image input, generate one sentence. This sentence overall must form a single coherent narative story. Generate a story with an enhanced Structure aspect and organize the story using a clear beginning, middle, and end structure.\"\n",
    "    processed_images = []\n",
    "    for img_filename in story_images:\n",
    "        img_path = os.path.join(image_dir, img_filename)\n",
    "        try:\n",
    "            raw_image = Image.open(img_path).convert('RGB')\n",
    "            raw_image = raw_image.resize(resize_to, Image.LANCZOS)\n",
    "            processed_images.append(raw_image)\n",
    "        except:\n",
    "            pass\n",
    "    if not processed_images:\n",
    "        return [], \"No valid images were processed.\"\n",
    "    conversation_content = [{\"type\": \"image\"} for _ in processed_images]\n",
    "    conversation_content.append({\"type\": \"text\", \"text\": query_tmp})\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": conversation_content,\n",
    "        }\n",
    "    ]\n",
    "    text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    inputs = processor(\n",
    "        text=[text_prompt],\n",
    "        images=processed_images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    inputs = {k: v.to(device, non_blocking=True) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=False\n",
    "        )\n",
    "    generated_ids = [\n",
    "        output_id[len(input_id):]\n",
    "        for input_id, output_id in zip(inputs['input_ids'], output_ids)\n",
    "    ]\n",
    "    story_description = processor.batch_decode(\n",
    "        generated_ids,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )[0]\n",
    "    return processed_images, story_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForVision2Seq, AutoTokenizer, AutoImageProcessor, StoppingCriteria\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def apply_prompt_template(prompt):\n",
    "    s = (\n",
    "            '<|system|>\\nA chat between a curious user and an artificial intelligence assistant. '\n",
    "            \"The assistant gives helpful, detailed, and polite answers to the user's questions.<|end|>\\n\"\n",
    "            f'<|user|>\\n<image>\\n{prompt}<|end|>\\n<|assistant|>\\n'\n",
    "        )\n",
    "    return s\n",
    "\n",
    "class EosListStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, eos_sequence = [32007]):\n",
    "        self.eos_sequence = eos_sequence\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        last_ids = input_ids[:,-len(self.eos_sequence):].tolist()\n",
    "        return self.eos_sequence in last_ids      \n",
    "\n",
    "def generate_story_from_image_file(image_filename, query, image_dir=\"images/val/\"):\n",
    "\n",
    "    model_name_or_path = \"Salesforce/xgen-mm-phi3-mini-instruct-r-v1\"\n",
    "    with torch.inference_mode():\n",
    "        model = AutoModelForVision2Seq.from_pretrained(model_name_or_path, trust_remote_code=True, torch_dtype=torch.float16)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True, use_fast=False, legacy=False)\n",
    "        image_processor = AutoImageProcessor.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "        tokenizer = model.update_special_tokens(tokenizer)\n",
    "\n",
    "        model = model.cuda()\n",
    "        raw_images = []\n",
    "        for filename in image_filename:\n",
    "            print(\"image_dir:\" + image_dir)\n",
    "            print(\"filename:\" + filename)\n",
    "            image_path = os.path.join(image_dir, filename)\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "            # raw_image = img.resize((224, 224), Image.LANCZOS)\n",
    "            raw_images.append(img)\n",
    "            \n",
    "        inputs = image_processor(raw_images, return_tensors=\"pt\", image_aspect_ratio='anyres')\n",
    "        prompt = apply_prompt_template(query)\n",
    "        language_inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "        inputs.update(language_inputs)\n",
    "        inputs = {name: tensor.cuda() for name, tensor in inputs.items()}\n",
    "        image_sizes = [img.size for img in raw_images]\n",
    "        generated_text = model.generate(\n",
    "            **inputs,\n",
    "            image_size=image_sizes,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=128,\n",
    "            top_p=None,\n",
    "            num_beams=1,\n",
    "            stopping_criteria=[EosListStoppingCriteria()],\n",
    "        )\n",
    "        prediction = tokenizer.decode(generated_text[0], skip_special_tokens=True).split(\"<|end|>\")[0]\n",
    "        return prediction\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_story_ids = story_ids[:20]\n",
    "\n",
    "for story_id in temp_story_ids:\n",
    "    story_images = get_story_images(story_id, annotations_dict, images, image_folder)\n",
    "\n",
    "    story_annotations = sorted(\n",
    "        annotations_dict.get(story_id, []),\n",
    "        key=lambda ann: ann.worker_arranged_photo_order\n",
    "    )\n",
    "    captions = \" \".join([annotation.original_text for annotation in story_annotations])\n",
    "    # query_generate = \"You will be given a sequence of images. For each image input, generate one sentence resulting in a 5 sentence story. Generate a humanlike story with a focus in the character development where the image sequence must generate an impact on the characters inside with a climax that is inevitable and satisfying for the characters.\"\n",
    "    # query_enhance = \"You will be given a sequence of images and a storytelling caption of that image, based on this caption: '{captions}, Generate a humanlike story with a focus in the character development where the image sequence must generate an impact on the characters inside with a climax that is inevitable and satisfying for the characters.\"\n",
    "    # query_generate = \"You will be given a sequence of images. For each image input, generate one sentence. Generate a humanlike story with a focus in the character development where the image sequence must generate an impact on the characters inside. Create a climax that is inevitable and satisfying for the characters. This climax must be unexpected, happening in a way the audience could not have anticipated.\"\n",
    "    # query_enhance = \"You will be given a sequence of images and a storytelling caption of that image, based on this caption: '{captions}, Generate a humanlike story with a focus in the character development where the image sequence must generate an impact on the characters inside. Create a climax that is inevitable and satisfying for the characters. This climax must be unexpected, happening in a way the audience could not have anticipated.\"\n",
    "    # query_generate = \"You will be given a sequence of images. For each image input, generate one sentence. Generate a humanlike story with an enhanced structure aspect and organize the story using a clear beginning, middle, and end structure.\"\n",
    "    # query_enhance = f\"You will be given a sequence of images and a storytelling caption of that image, based on this caption: '{captions}, Generate a humanlike story with an enhanced structure aspect and organize the story using a clear beginning, middle, and end structure.\"\n",
    "    # query_generate = \"You will be given a sequence of images. For each image input, generate one short sentence resulting in a 5 sentence story. Keep each sentence in the story precise and concise. Generate a story with a consistent World Building of the story. The world must have its own rules and logic. Ensure that the world feels real to the reader while they are within it. \"\n",
    "    # query_enhance = \"You will be given a sequence of images and a story caption of that image, based on this caption: '{captions}, generate one short sentence for each image input, resulting in a 5 sentence story. Generate a story with a consistent World Building of the story. The world must have its own rules and logic. Ensure that the world feels real to the reader while they are within it. Keep each sentence in the story precise and concise.\"\n",
    "    \n",
    "    # query_generate = \"You will be given a sequence of images. For each image input, generate one sentence. Generate a humanlike story with a proper immersion aspect. A good immersion is a story that has a consistent World Building of the story. The world must have its own rules and logic. Ensure that the world feels real to the reader while they are within it.\"\n",
    "\n",
    "    # query_generate = \"You will be given a sequence of images. For each image input, generate one short sentence resulting in a 5 sentence story. Keep each sentence in the story precise and concise. Variate between a first person and third person story. You can generate a named entity for the entities detected in the image. Generate a story with a consistent World Building of the story. The world must have its own rules and logic. Ensure that the world feels real to the reader while they are within it. \"\n",
    "    # query_enhance = \"You will be given a sequence of images and a story caption of that image, based on this caption: '{captions}, generate one short sentence for each image input, resulting in a 5 sentence story. Variate between a first person and third person story. You can generate a named entity for the entities detected in the image. Generate a story with a consistent World Building of the story. The world must have its own rules and logic. Ensure that the world feels real to the reader while they are within it. Keep each sentence in the story precise and concise.\"\n",
    "    # query_generate = 'You will be given a sequence of images. Generate a story with a proper immersion and structure aspect for each image input, resulting in a 5 sentence story. A good immersion is a story that has a consistent World Building of the story. The world must have its own rules and logic. Ensure that the world feels real to the reader while they are within it. A good structure aspect and organized the story using a clear beginning, middle, and end structure. Variate between a first person and third person story. You can generate a named entity for the entities detected in the image.'\n",
    "\n",
    "    # query_enhance = 'You will be given a sequence of images and a story caption as the story context foundation. The foundation story is: “{captions}”. Generate a story with a proper immersion and structure aspect for each image input, resulting in a 5 sentence story. A good immersion is a story that has a consistent World Building of the story. The world must have its own rules and logic. Ensure that the world feels real to the reader while they are within it. A good structure aspect and organized the story using a clear beginning, middle, and end structure. Variate between a first person and third person story. You can generate a named entity for the entities detected in the image.'\n",
    "    # query_generate = 'You will be given a sequence of images. Generate one sentence for each image input, resulting in a 5 sentence story. Variate between a first person and third person story. You can generate a named entity for the entities detected in the image. Generate a humanlike story with a proper immersion and structure aspect. A good immersion is a story that has a consistent World Building of the story. The world must have its own rules and logic. Ensure that the world feels real to the reader while they are within it. A good structure aspect and organized the story using a clear beginning, middle, and end structure'\n",
    "\n",
    "    # query_enhance = 'You will be given a sequence of images and a story caption as the story context foundation. The foundation story is: “{captions}”. Generate one sentence for each image input, resulting in a 5 sentence story. Variate between a first person and third person story. You can generate a named entity for the entities detected in the image. Generate a humanlike story with a proper immersion and structure aspect. A good immersion is a story that has a consistent World Building of the story. The world must have its own rules and logic. Ensure that the world feels real to the reader while they are within it. A good structure aspect and organized the story using a clear beginning, middle, and end structure'\n",
    "    \n",
    "    # query_generate = 'You will be given a sequence of images. Generate one sentence for each image input, resulting in a 5 sentence story. Variate between a first person and third person story. You can generate a named entity for the entities detected in the image. Generate a humanlike story with a proper immersion and structure aspect. A good immersion is a story that has a consistent World Building of the story. The world must have its own rules and logic. Ensure that the world feels real to the reader while they are within it. A good structure aspect and organized the story using a clear beginning, middle, and end structure'\n",
    "    # query_enhance = 'You will be given a sequence of images and a story caption as the story context foundation. The foundation story is: “{captions}”. Generate one sentence for each image input, resulting in a 5 sentence story. Variate between a first person and third person story. You can generate a named entity for the entities detected in the image. Generate a humanlike story with a proper immersion and structure aspect. A good immersion is a story that has a consistent World Building of the story. The world must have its own rules and logic. Ensure that the world feels real to the reader while they are within it. A good structure aspect and organized the story using a clear beginning, middle, and end structure'\n",
    "    # query_generate = 'You will be given a sequence of images. Generate one sentence for each image input, resulting a 5 sentence story. The story must be a humanlike story with a proper immersion and structure aspect. A good immersion is a story that has a consistent World Building of the story. The world must have its own rules and logic. Ensure that the world feels real to the reader while they are within it. A good structure aspect and organized the story using a clear beginning, middle, and end structure. Variate between a first person and third person story. You can generate a named entity for the entities detected in the image. '\n",
    "    # query_enhance = 'You will be given a sequence of images and a story caption as the story context foundation. The foundation story is: “{captions}”. Generate one sentence for each image input, resulting a 5 sentence story. The story must be a humanlike story with a proper immersion and structure aspect. A good immersion is a story that has a consistent World Building of the story. The world must have its own rules and logic. Ensure that the world feels real to the reader while they are within it. A good structure aspect and organized the story using a clear beginning, middle, and end structure. Variate between a first person and third person story. You can generate a named entity for the entities detected in the image.'\n",
    "    # query_generate = \"You will be given a sequence of images. For each image input, generate one sentence. Generate a humanlike story with a proper immersion and structure aspect. A good immersion is a story that has a consistent World Building of the story. The world must have its own rules and logic. Ensure that the world feels real to the reader while they are within it. A good structure aspect and organized the story using a clear beginning, middle, and end structure\"\n",
    "    # query_generate = 'Generate one sentence that is most relevant for each image input. This sentence combined must form a humanlike story with an enhanced aspect of immersion and structure.\n",
    "    query_generate = 'You will be given a sequence of images. Generate one sentence for each image input, resulting a 5 sentence story where each sentence must correlate with their respected image input. Make sure the story is a 5 sentence story. The story must be humanlike with a proper immersion and structure aspect. A proper immersion is a story that has a consistent World Building of the story. The world must have its own rules and logic. Ensure that the world feels real to the reader while they are within it. A proper structure aspect and organized the story using a clear beginning, middle, and end structure. Variate between a first person and third person story. You can generate a named entity for the entities detected in the image.'\n",
    "  \n",
    "    # query_generate = 'Generate one sentence that is most relevant for each image input. This sentence combined must form a humanlike story with an enhanced aspect of immersion and structure. A proper immersion is a story that has a consistent world-building that have its own consistent rules and logic, ensuring that the world feels real to the reader while they are within it. A proper structure aspect is a story that has a clear beginning, middle and end structure. Variate the story between a first person and third person story. You can also generate a named entity for the entities detected in the image'\n",
    "    # processed_images, generated_story = qwen_test_all_in_one(story_images, image_dir=image_folder, query_tmp = query_generate)\n",
    "    # _, enhanced_story = qwen_test_all_in_one(story_images, image_dir=image_folder, query_tmp = query_enhance)\n",
    "\n",
    "    blip_query = 'Given this story \"{generated_story}\", do you think it contains a good immersion? A good immersion is a story with a consistent World Building of the story. The world must have its own rules and logic. Ensure that the world feels real to the reader while they are within it. Give me your judgement and reason'\n",
    "    # blip_query = 'Given the input image story and also this story \"{generated_story}\", do you think that it is a good story with respect to the input image? Why or why not? Consider the criteria that each sentence must properly explain the related image. So the first sentence must explain the first image, second sentence must explain the second image, and so on. Also consider the aspect of immersion. A good immersion is a story that has a consistent World Building of the story. The world must have its own rules and logic. Ensure that the world feels real to the reader while they are within it.'\n",
    "    blipjudgement = generate_story_from_image_file(image_filename= [], query=blip_query)\n",
    "    # blip_query = 'Given the input image story and also this story \"{generated_story}\", evaluate if the immersion aspect is good or not, also give the reason for that evaluation. Evaluate only the text story and not the image sequence. A good immersion is a story that has a consistent World Building of the story. The world must have its own rules and logic. Ensuring that the world feels real to the reader while they are within it.'\n",
    "    # _, model_judgement = qwen_test_all_in_one(story_images, image_dir=image_folder, query_tmp = blip_query)\n",
    "    print(blipjudgement)\n",
    "\n",
    "    # plt.figure(figsize=(20, 10))\n",
    "    # num_images = len(processed_images)\n",
    "    # for i, img in enumerate(processed_images, 1):\n",
    "    #     plt.subplot(2, num_images, i)\n",
    "    #     plt.imshow(img)\n",
    "    #     plt.axis('off') \n",
    "    #     plt.title(f'Image {i}')\n",
    "    # plt.subplot(2, 1, 2)\n",
    "    # plt.text(0.5, 0.7, \"Original: \"+ captions, horizontalalignment='center', verticalalignment='center', wrap=True, fontsize=10, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    # plt.text(0.5, 0.3, \"Generated: \"+ generated_story, horizontalalignment='center', verticalalignment='center', wrap=True, fontsize=10, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    # plt.text(0.5, 0 , \"Prompt: \"+ query_generate, horizontalalignment='center', verticalalignment='center', wrap=True, fontsize=10, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    # plt.axis('off')\n",
    "    # plt.title(f'Story Description for Story ID: {story_id}')\n",
    "    # plt.tight_layout()\n",
    "\n",
    "    # timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    # combined_filename = f'story_combined_{timestamp}_{uuid.uuid4().hex}.png'\n",
    "    # folder_name = \"testing\"\n",
    "    # os.makedirs(folder_name, exist_ok=True)\n",
    "    # plt.savefig(os.path.join(folder_name, combined_filename))\n",
    "\n",
    "    # plt.show()\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Template code from source website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForVision2Seq, AutoTokenizer, AutoImageProcessor, StoppingCriteria\n",
    "# import torch\n",
    "# import requests\n",
    "# from PIL import Image\n",
    "\n",
    "# # define the prompt template\n",
    "# def apply_prompt_template(prompt):\n",
    "#     s = (\n",
    "#             '<|system|>\\nA chat between a curious user and an artificial intelligence assistant. '\n",
    "#             \"The assistant gives helpful, detailed, and polite answers to the user's questions.<|end|>\\n\"\n",
    "#             f'<|user|>\\n<image>\\n{prompt}<|end|>\\n<|assistant|>\\n'\n",
    "#         )\n",
    "#     return s \n",
    "# class EosListStoppingCriteria(StoppingCriteria):\n",
    "#     def __init__(self, eos_sequence = [32007]):\n",
    "#         self.eos_sequence = eos_sequence\n",
    "\n",
    "#     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "#         last_ids = input_ids[:,-len(self.eos_sequence):].tolist()\n",
    "#         return self.eos_sequence in last_ids      \n",
    "\n",
    "# # load models\n",
    "# model_name_or_path = \"Salesforce/xgen-mm-phi3-mini-instruct-r-v1\"\n",
    "# model = AutoModelForVision2Seq.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True, use_fast=False, legacy=False)\n",
    "# image_processor = AutoImageProcessor.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "# tokenizer = model.update_special_tokens(tokenizer)\n",
    "\n",
    "# # craft a test sample\n",
    "# img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n",
    "# raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "# query = \"how many images that you take as an input?\"\n",
    "\n",
    "# model = model.cuda()\n",
    "# inputs = image_processor([raw_image,raw_image,raw_image,raw_image,raw_image], return_tensors=\"pt\")\n",
    "# prompt = apply_prompt_template(query)\n",
    "# language_inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "# inputs.update(language_inputs)\n",
    "# inputs = {name: tensor.cuda() for name, tensor in inputs.items()}\n",
    "# generated_text = model.generate(**inputs, image_size=[raw_image.size],\n",
    "#                                 pad_token_id=tokenizer.pad_token_id,\n",
    "#                                 do_sample=False, max_new_tokens=768, top_p=None, num_beams=1,\n",
    "#                                 stopping_criteria = [EosListStoppingCriteria()],\n",
    "#                                 )\n",
    "# prediction = tokenizer.decode(generated_text[0], skip_special_tokens=True).split(\"<|end|>\")[0]\n",
    "# print(\"==> prediction: \", prediction)\n",
    "# # output: ==> prediction: There is one dog in the picture.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hansel_vist_3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
